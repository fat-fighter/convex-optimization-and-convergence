@article{gd-overview,
	author    = {Sebastian Ruder},
	title     = {An overview of gradient descent optimization algorithms},
	journal   = {CoRR},
	volume    = {abs/1609.04747},
	year      = {2016},
	url       = {http://arxiv.org/abs/1609.04747},
	archivePrefix = {arXiv},
	eprint    = {1609.04747},
	timestamp = {Wed, 07 Jun 2017 14:40:06 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/Ruder16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gd-original,
	author	= {M. Augustine Cauchy},
	title	= {M\'ethode g\'en\'erale pour la r\'esolution des syst'emes d'\'equations simultan\'ees},
	year	= {1847},
	joural	= {C. R. Acad. Sci. Paris}
}

@article{adam,
	author    = {Diederik P. Kingma and Jimmy Ba},
	title     = {Adam: {A} Method for Stochastic Optimization},
	journal   = {CoRR},
	volume    = {abs/1412.6980},
	year      = {2014},
	url       = {http://arxiv.org/abs/1412.6980},
	archivePrefix = {arXiv},
	eprint    = {1412.6980},
	timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{non-adam,
	title={On the Convergence of Adam and Beyond},
	author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
	booktitle={International Conference on Learning Representations},
	year={2018},
	url={https://openreview.net/forum?id=ryQu7f-RZ},
}

@article{sgd,
  author    = {Jaivardhan Kapoor},
  title     = {Lecture 18 - Sparse Recovery II},
  journal   = {Topics in Learning Theory, CS777, Indian Institute of Technology Kanpur},
  year      = {2018}
}

@article{gd,
  author    = {Afroz Alam},
  title     = {Lecture 15 - Optimization Techniques II},
  journal   = {Topics in Learning Theory, CS777, Indian Institute of Technology Kanpur},
  year      = {2018}
}

@article{adagrad,
    Author = {Duchi, John and Hazan, Elad and Singer, Yoram},
    Title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2010},
    Month = {Mar},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-24.html},
    Number = {UCB/EECS-2010-24},
    Abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods significantly outperform state-of-the-art, yet non-adaptive, subgradient algorithms.}
}
